{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #3\n",
    "\n",
    "**Michael Carrion, Rahul Rangwani, Vanessa Ma**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this problem set we are going to make use of `pandas` to analyze the effect of a fictious experiment I have added to a data set. The data we will be using is the sample data provided by Yelp. The goal is to become familiar\n",
    "to working with such dataset.\n",
    "\n",
    "The original data is available here: [Yelp data](https://www.yelp.com/dataset/download).\n",
    "For this homework you will use the data I constructed using the original sample. You can download such file here: \n",
    "\n",
    " - **homework data**: [hw-yelp-data.zip](http://econ21340.lamadon.com/hw-yelp.tar.gz ) (~2.6Go)\n",
    "\n",
    "In the data I introduced an experiment. The back story is that Yelp rolled out a new interface for a randomly selected group of users. These users were randomly selected among those that posted a review in the month of January 2010. The `id` of these users are listed in the `yelp_academic_dataset_review_treatment.json` file present in the archive.\n",
    "\n",
    "For this group of users, a new website interface was put in place on February 1st 2010. As a Yelp budding data scientist/economist, you are asked to analyze the impact of the new app interface. The company is interested in estimating its effect on user engagement, measured by rating activities. We will focus mainly on the number of ratings for each user. \n",
    "\n",
    "In this homework we will cover:\n",
    " 1. loading large dataset using streaming/chunks, learn about json\n",
    " - working with date in pandas\n",
    " - analyze randomly assigned treatment\n",
    " - construct comparable control group\n",
    " - analyze at the level of randomization\n",
    " \n",
    "some useufl links:\n",
    " - [tutorial on dates in pandas](https://pbpython.com/pandas-grouper-agg.html)\n",
    " - [pandas documentation on reshaping](https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html)\n",
    " - [yelp data documentation](https://www.yelp.com/dataset/documentation/main)\n",
    "\n",
    "We start with a simple list of imports, as well as defining the path to the file we will be using. Please update the paths to point to the correct location on your computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T01:48:33.933316Z",
     "start_time": "2020-05-20T01:48:33.861632Z"
    }
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.modules[__name__].__dict__.clear()\n",
    "import os\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "# file_review = os.path.expanduser(\"~/Downloads/yelp_academic_dataset_review_experiment.json\")\n",
    "# file_treatment = os.path.expanduser(\"~/Downloads/yelp_academic_dataset_review_treatment.json\")\n",
    "file_review = \"data/yelp_academic_dataset_review_experiment.json\"\n",
    "file_treatment = \"data/yelp_academic_dataset_review_treatment.json\"\n",
    "\n",
    "def file_len(fname):\n",
    "    \"\"\" Function which efficiently computes the number of lines in file\"\"\"\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are already familiar with the following section, this is the code that loads my solution. Since you don't have the file, this part of the code won't work for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T01:48:36.202312Z",
     "start_time": "2020-05-20T01:48:36.148758Z"
    }
   },
   "outputs": [],
   "source": [
    "# %cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "#import solutions.sol_pset3 as solution # you need to command this, you don't have the solution file!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Loading the yelp review data\n",
    "\n",
    "The data is stored in `json` format. This is a widely used format to store structured data. See [here](https://www.w3schools.com/python/python_json.asp) for working with json in general in python.\n",
    "\n",
    "The data itself is quite large, hence we are going to use `chunksize` argument of the `read_json` function in `pandas`. You can of course try for your self to directly load the data using `pd.read_json(file_review)`,however this might take a while!\n",
    "\n",
    "In the following section, I provided an example of the code that loads the business information using chunks of size `100,000`. The code **contains a few errors**. Use the data documentation (using the link in the intro) to fix the code of loading the data. The provided code also drops variables which will be very needed and keep some others that are just going to clutter your computer memory. Again, look at the documentation and at the questions ahead to keep the right set of variables.\n",
    "\n",
    "\n",
    "Note how the code first compute the length of the file \n",
    "\n",
    "```python\n",
    "size = 100000\n",
    "review = pd.read_json(filepath, lines=True,\n",
    "                      dtype={'review_id':str,\n",
    "                             'user_id':float,\n",
    "                             'business_id':str,\n",
    "                             'stars':int,\n",
    "                             'date':str,\n",
    "                             'text':float,\n",
    "                             'useful':int,\n",
    "                             'funny':str,\n",
    "                             'cool':int},\n",
    "                      chunksize=size)\n",
    "\n",
    "chunk_list = []\n",
    "for chunk_review in tqdm.tqdm(review,total=  np.ceil(file_len(filepath)/size )  ):\n",
    "    # Drop columns that aren't needed\n",
    "    chunk_review = chunk_review.drop(['review_id','date'], axis=1)\n",
    "    chunk_list.append(chunk_review)\n",
    "\n",
    "df = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)\n",
    "```\n",
    "\n",
    "The following runs my version of the code, it takes around 2 minutes on my laptop. I show you a few of the columns that I chose to extract. In particular, you can check that you get the right row count of `7998013`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question1(filepath):\n",
    "    size = 100000\n",
    "    review = pd.read_json(filepath, lines=True,\n",
    "                          dtype={'review_id':str,\n",
    "                                 'user_id':str,\n",
    "                                 'business_id':str,\n",
    "                                 'stars':int,\n",
    "                                 'date':str,\n",
    "                                 'text':str,\n",
    "                                 'useful':int,\n",
    "                                 'funny':int,\n",
    "                                 'cool':int},\n",
    "                          chunksize=size)\n",
    "\n",
    "    chunk_list = []\n",
    "    for chunk_review in tqdm.tqdm(review,total=  np.ceil(file_len(filepath)/size )  ):\n",
    "        # Drop columns that aren't needed\n",
    "        chunk_review.drop(['business_id', 'stars','text','useful','funny','cool'], axis=1, inplace = True)\n",
    "        chunk_list.append(chunk_review)\n",
    "        df = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)\n",
    "    return df\n",
    "\n",
    "### For Vanessa Only\n",
    "def question1_lm(filepath):\n",
    "    size = 5000\n",
    "    chunk_count = 0\n",
    "    chunk_max = 3\n",
    "    review = pd.read_json(filepath, lines=True,\n",
    "                          dtype={'review_id':str,\n",
    "                                 'user_id':str,\n",
    "                                 'business_id':str,\n",
    "                                 'stars':int,\n",
    "                                 'date':str,\n",
    "                                 'text':str,\n",
    "                                 'useful':int,\n",
    "                                 'funny':int,\n",
    "                                 'cool':int},\n",
    "                          chunksize=size)\n",
    "\n",
    "    chunk_list = []\n",
    "    for chunk_review in tqdm.tqdm(review,total=  np.ceil(file_len(filepath)/size )  ):\n",
    "        chunk_count += 1\n",
    "        if chunk_count <= chunk_max:\n",
    "        # Drop columns that aren't needed\n",
    "            chunk_review.drop(['business_id', 'stars','text','useful','funny','cool'], axis=1, inplace = True)\n",
    "            chunk_list.append(chunk_review)\n",
    "        df = pd.concat(chunk_list, ignore_index=True, join='outer', axis=0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T01:51:00.630840Z",
     "start_time": "2020-05-20T01:48:43.485017Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_review = question1(file_review)\n",
    "df_review['date'] = pd.to_datetime(df_review.date) # convert the date string to an actual date\n",
    "df_review[['review_id','user_id','date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = question1_lm(file_review)\n",
    "df_test['date'] = pd.to_datetime(df_test.date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our first plot of the data\n",
    "\n",
    "Next, to get a sense of the data, we plot user engagement over time. For this I ask you to plot the log number of reviews per month using our created data. \n",
    "\n",
    "To get the plot, I recommend you look into either the `resample` menthod or the `grouper` method. If you are not too familiar with them, I added a link at the top to a great tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question2(df_review):\n",
    "    mod = df_review.groupby(pd.Grouper(key='date', freq='M')).count()\n",
    "    plt.plot(mod,color=\"tab:blue\")\n",
    "    plt.ylabel(\"user_id\")\n",
    "    plt.xlabel('date')\n",
    "    plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2(df_review) #Note: we use log scaling of matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A randomized experiment\n",
    "\n",
    "We now want to extract our experimental data from our large data set. Given the random assignment we are going to compare the treated group to simply everyone else in the data. In this exercice, we are interested in the effect of the policy over time. We are then going to look at the log number of reviews in each of the months around the introduction of the interface change.\n",
    "\n",
    "I would like for you to do the following:\n",
    " 1. extract the list of treated individuals from the provided file\n",
    " 2. attach the treated status to each observation in the data, you can use `eval` or a merge.\n",
    " 3. plot the log number of reviews per month in the treatment and in the control group. \n",
    " 4. given that the treatment status was randomized, the picture should look a bit surpising, please explain what you would have expected to see.\n",
    " \n",
    "Here is the plot I get, try to reproduce it as closely as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treatment = pd.read_json(file_treatment, lines=True, dtype={'user_id':str})\n",
    "treatment = treatment.rename(columns={0: \"treatment\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question3(df_review,file_treatment):\n",
    "    merged = df_review.merge(treatment,left_on='user_id',right_on='treatment',how='outer')\n",
    "    merged.treatment = merged.treatment.notnull()\n",
    "    \n",
    "    #Plotting\n",
    "    untreated = merged.loc[merged.treatment == False]\n",
    "    treated = merged.loc[merged.treatment == True]\n",
    "    un = untreated.groupby(pd.Grouper(key='date', freq='M')).count()\n",
    "    treat = treated.groupby(pd.Grouper(key='date', freq='M')).count()\n",
    "    un = un.loc[(un!=0).any(axis=1)]\n",
    "    treat = treat.loc[(treat!=0).any(axis=1)]\n",
    "    plt.plot(un.iloc[:,0],color=\"tab:blue\",label = \"Untreated\")\n",
    "    plt.plot(treat.iloc[:,0],color = 'tab:orange', label = \"Treated\")\n",
    "    plt.ylabel(\"review_id\")\n",
    "    plt.xlabel('date')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    return treatment, merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Vanessa only\n",
    "user_treat, merged = question3(df_test, file_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_treat, merged = question3(df_review,file_treatment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the treatment was random, we would expect to see the number of reviews for both the treatment and control group follow a similar trend over time. However, we see the number of reviews posted in treatment group decline sharply after the beginning of 2010, suggesting that this is not a great comparison group for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing comparison group\n",
    "\n",
    "We clearly created some issues in the way we analyzed our sample. In this section we are going to use a more comparable group. \n",
    "\n",
    " 1. using the criteria described in the intro, construct the original set of users from which the treatment group was selected. \n",
    " - extracts the users from the this group wich are not in the treatment group - this will be our control group.\n",
    " - using this new control group, plot the log number of reviews in each month for treatment and control\n",
    " - finally plot the differences of the outcome, however make sure to remove (subtract) the log of the number of individuals in each group to obtain the plot of the log number of reviews per user - otherwise your intercept won't be around 0!\n",
    " \n",
    "Here are the plots I got: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###QUESTION 4###\n",
    "\n",
    "# 4.1 Construct the original set of users from which the treatment group was selected\n",
    "jan10 = df_review.loc[(df_review.date < '2010-02-01') & (df_review.date >= '2010-01-01')]\n",
    "merged_new = jan10.merge(treatment,left_on='user_id',right_on='treatment',how='outer')\n",
    "merged_new.treatment = merged_new.treatment.notnull()\n",
    "\n",
    "treatment_group = pd.DataFrame(merged_new.loc[merged_new.treatment == True].user_id)\n",
    "treatment_total = df_review.merge(treatment_group,on = 'user_id',how = 'right')\n",
    "treat = treatment_total.groupby(pd.Grouper(key='date', freq='Q')).nunique()\n",
    "# treat = treat.loc[(treat!=0).any(axis=1)]\n",
    "treat.drop(columns = [\"date\"], inplace = True)\n",
    "\n",
    "# 4.2 Construct control group\n",
    "control_group = pd.DataFrame(merged_new.loc[merged_new.treatment == False].user_id)\n",
    "control_total = df_review.merge(control_group,on = 'user_id',how = 'right')\n",
    "un = control_total.groupby(pd.Grouper(key='date', freq='Q')).nunique()\n",
    "# un = un.loc[(un!=0).any(axis=1)]\n",
    "un.drop(columns = [\"date\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Vanessa only\n",
    "###QUESTION 4\n",
    "\n",
    "# 4.1 Construct the original set of users from which the treatment group was selected\n",
    "jan10 = df_test.loc[(df_test.date < '2010-02-01') & (df_test.date >= '2010-01-01')]\n",
    "merged_new = jan10.merge(treatment,left_on='user_id',right_on='treatment',how='outer')\n",
    "merged_new.treatment = merged_new.treatment.notnull()\n",
    "\n",
    "treatment_group = pd.DataFrame(merged_new.loc[merged_new.treatment == True].user_id)\n",
    "treatment_total = df_test.merge(treatment_group,on = 'user_id',how = 'right')\n",
    "treat = treatment_total.groupby(pd.Grouper(key='date', freq='Q')).nunique()\n",
    "# treat = treat.loc[(treat!=0).any(axis=1)]\n",
    "treat.drop(columns = [\"date\"], inplace = True)\n",
    "# treat[\"treat_diff\"] = treat[\"review_id\"] - treat[\"user_id\"]\n",
    "\n",
    "# 4.2 Construct control group\n",
    "control_group = pd.DataFrame(merged_new.loc[merged_new.treatment == False].user_id)\n",
    "control_total = df_test.merge(control_group,on = 'user_id',how = 'right')\n",
    "un = control_total.groupby(pd.Grouper(key='date', freq='Q')).nunique()\n",
    "# un = un.loc[(un!=0).any(axis=1)]\n",
    "un.drop(columns = [\"date\"], inplace = True)\n",
    "# un[\"untreated_diff\"] = treat[\"review_id\"] - treat[\"user_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3: plot number of reviews for each group\n",
    "plt.figure()\n",
    "plt.plot(un.index, un[\"review_id\"], color = \"tab:blue\", label = \"Untreated\")\n",
    "plt.plot(treat.index, treat[\"review_id\"], color = \"tab:orange\", label = \"Treated\")\n",
    "plt.ylabel(\"review_id\")\n",
    "plt.xlabel('date')\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.4: plot number of reviews for control and treatment group\n",
    "dstart = datetime.datetime(2008,1,1)\n",
    "dend = datetime.datetime(2020,6,1)\n",
    "\n",
    "mod = un.merge(treat,left_index=True,right_index=True,how=\"inner\", suffixes = (\"_control\", \"_treat\"))\n",
    "mod['control_diff'] = np.log(mod['review_id_control']) - np.log(mod['user_id_control'])\n",
    "mod['treated_diff'] = np.log(mod['review_id_treat']) - np.log(mod['user_id_treat'])\n",
    "mod['diff'] = mod['treated_diff']-mod['control_diff'] \n",
    "plt.figure()\n",
    "plt.plot(mod.index,mod['diff'])\n",
    "plt.xlim(dstart, dend)\n",
    "plt.ylim(-.6,0.5)\n",
    "plt.ylabel(\"diff\")\n",
    "plt.xlabel('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:19:06.689725Z",
     "start_time": "2020-05-20T02:19:03.377371Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_local = solution.question4(df_local,user_treat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using activity per user and time\n",
    "\n",
    "We are now interested in conducting some inference on our results. However we remember that the level of randomization is at the `user` and not at the `review`. Hence we now decide to construct observations at the `(user,year)` level. We decide to use years instead of months because the probability at the month level is too low.\n",
    "\n",
    " 1. Construct a DataFrame with all `(user,year)` pairs and a column called `post` which is equal to 1 if the user posted in that year and 0 if he didn't. To construct such dataframe I used the `pd.MultiIndex.from_product` function, but one could use a `merge` instead.\n",
    " 2. Use this newly created DataFrame to plot the level for each group, and to plot the difference between the two.\n",
    " \n",
    "Here are the plots I constructed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Construct dataframe with user, year pairs, post binary column if user posted in that year, and 0 if not\n",
    "treatment_total[\"year\"] = treatment_total[\"date\"].dt.year + 1\n",
    "control_total[\"year\"] = control_total[\"date\"].dt.year + 1\n",
    "\n",
    "if treatment_total[\"year\"].nunique() > control_total[\"year\"].nunique():\n",
    "    year_list = treatment_total[\"year\"].unique()\n",
    "else:\n",
    "    year_list = control_total[\"year\"].unique()\n",
    "        \n",
    "\n",
    "# Construct Treatment Part of the dataframe\n",
    "treat_index = pd.MultiIndex.from_product([treatment_total[\"user_id\"].unique(), year_list]).to_frame()\n",
    "\n",
    "treat_tab = treatment_total.groupby([\"user_id\", \"year\"]).count()\n",
    "# treat_tab[\"treated\"] = 1\n",
    "treat_tab[\"post\"] = treat_tab[\"review_id\"] > 0 \n",
    "\n",
    "user_year_treat = treat_index.merge(treat_tab, how = \"left\", right_on = [\"user_id\", \"year\"], left_index = True, suffixes = (\"\", \"_treat\"))\n",
    "user_year_treat.fillna(0, inplace = True)\n",
    "user_year_treat[\"treated\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_year_treat"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct Control Part of the dataframe\n",
    "control_index = pd.MultiIndex.from_product([control_total[\"user_id\"].unique(), year_list]).to_frame()\n",
    "\n",
    "control_tab = control_total.groupby([\"user_id\", \"year\"]).count()\n",
    "# control_tab[\"treated\"] = 0\n",
    "control_tab[\"post\"] = control_tab[\"review_id\"] > 0 \n",
    "\n",
    "user_year_control = control_index.merge(control_tab, how = \"left\", right_on = [\"user_id\", \"year\"], left_index = True, suffixes = (\"\", \"\"))\n",
    "user_year_control.fillna(0, inplace = True)\n",
    "user_year_control[\"treated\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate Treatment and Control Parts of Dataframe\n",
    "df_user_year = pd.concat([user_year_treat, user_year_control], axis = 0, sort = False)\n",
    "df_user_year.rename(columns = {0: \"user\", 1: \"year_collected\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Plot Level for Treatment and Control Group, and Plot Difference\n",
    "df_user_year_tab = pd.DataFrame(df_user_year.groupby([\"treated\", \"year\"])[\"post\"].sum())\n",
    "df_user_year_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_prop_tab = df_user_year_tab.loc[0, :]\n",
    "treat_prop_tab = df_user_year_tab.loc[1, :]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(control_prop_tab.index, control_prop_tab[\"post\"]/control_total[\"user_id\"].nunique(), marker = \"o\", label = \"control\")\n",
    "plt.plot(treat_prop_tab.index, treat_prop_tab[\"post\"]/treatment_total[\"user_id\"].nunique(), marker = \"o\", label = \"treated\")\n",
    "plt.axvline(x = 2010, linestyle = \"--\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Activity Level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(control_prop_tab.index, treat_prop_tab[\"post\"]/treatment_total[\"user_id\"].nunique() - control_prop_tab[\"post\"]/control_total[\"user_id\"].nunique(), label = \"difference\")\n",
    "plt.axvline(x = 2010, linestyle = \"--\")\n",
    "plt.axhline(y = 0, linestyle = \"--\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Activity Level Difference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:41:43.687277Z",
     "start_time": "2020-05-20T02:41:40.619287Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_local_user = solution.question5(df_local,user_treat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:42:39.992305Z",
     "start_time": "2020-05-20T02:42:39.930219Z"
    }
   },
   "outputs": [],
   "source": [
    "df_local_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing confidence intervals\n",
    "\n",
    "\n",
    "In this final question our goal is to add some inference to our plot. We are going to simply use the asymptotic variance implied by the OLS formula. Do the following:\n",
    "\n",
    " 1. create a function that will take a dataframe containing the columns `post` and `treat` and returns the OLS $\\hat{\\beta}$ estimate of `post` on `treat` together with the estimate of the variance of $\\hat{\\beta}$ (Remember that in this simple case $\\hat{\\beta} = cov(y,x)/var(x)$ and that the variance is $\\sigma_{\\epsilon}^{2} / (n\\cdot var(x))$. Return the results as a new dataframe with one row and 2 columns.\n",
    " 2. apply your function to your data from question 5 for each `year` (you can do that using `pd.Grouper(freq='Y',key='date')` within a `groupby` and use the `apply` method.\n",
    " 3. use your grouped results to plot the estimates together with their 95% asymptotic confidence intervals\n",
    " 4. comment on the results, in particular on the dates before the start of the experiment.\n",
    " \n",
    "Below is the plot that I got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def numpy_ols(X, Y):\n",
    "    XX = np.matmul(X, np.transpose(X))\n",
    "    XY = np.matmul(X, Y)\n",
    "    beta_hat = np.linalg.solve(XX, XY)\n",
    "    return beta_hat\n",
    "\n",
    "def question6(df):\n",
    "#     reg = smf.ols(\"post ~ treat\", data = df).fit()\n",
    "    X = df[\"treated\"].to_numpy() #getting a key error here, something with a zero dimension array\n",
    "    Y = df[\"post\"].to_numpy()\n",
    "    beta_hat = numpy_ols(X, Y)\n",
    "    \n",
    "    sigma_ep = Y.std() - beta_hat*X  # standard deviation of Y - beta_hat * X\n",
    "    Xvar = X.var() # variance of X\n",
    "    n = 75840 # not sure if this is correct, since we are only passing through a year's worth of data each time\n",
    "    beta_var = sigma_ep^2 / (n*Xvar)\n",
    "    return pd.DataFrame(data = {\"beta_hat\": beta_hat, \"beta_var\": beta_var})\n",
    "\n",
    "# Trying something new\n",
    "\n",
    "def question6(df):\n",
    "    coef = df.groupby(\"year\").apply(ols)\n",
    "    plt.figure()\n",
    "    plt.errorbar(coefs.index, coefs.beta_hat, yerr = np.multiply(1.96, np.sqrt(coefs[\"var\"])))\n",
    "    return\n",
    "\n",
    "def ols(df):\n",
    "    cov = np.cov(df.post, df.treat)\n",
    "    beta_hat = cov_mat[0, 1] / cov[1,1]\n",
    "    r = np.subtract(df.post, np.multiply(beta_hat, df.treat))\n",
    "    beta_var = (np.std(resids) ** 2) / (len(df) * cov[1,1])\n",
    "    return beta_hat, beta_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_year_user = df_user_year.groupby([\"year\", \"treated\"]).apply(question6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:43:18.320091Z",
     "start_time": "2020-05-20T02:43:18.075998Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'errorbar.capsize': 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution.question6(df_local_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats, you are done!"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
